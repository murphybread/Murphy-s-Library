---
{"dg-publish":true,"description":"Implement RAG with FAISS using by Langchain","permalink":"/projects/library/400/430/430-00/430-00-c/","dgPassFrontmatter":true,"noteIcon":"0","created":"2024-02-17T11:44:21.811+09:00","updated":"2024-11-19T12:57:29.861+09:00"}
---

#[[Projects/Library/400/400\|400]]#ML_Engineer_Basic#[[Projects/Library/400/430/430\|430]]#ML_Development_Tools#[[Projects/Library/400/430/430.00/430.00\|430.00]]#LangChain#[[Projects/Library/400/430/430.00/430.00 c\|430.00 c]]#RAG_with_FAISS



### **1. 환경 설정 및 필요 라이브러리 설치**

```PY
# 필요한 라이브러리 설치
!pip install -q openai langchain tiktoken faiss-cpu
!pip install pydantic==1.10.13 docarray==0.32.1
!pip install -U langchain-openai

```
---

### **2. 기본 모듈과 API 키 설정**

```PY
import os
from google.colab import userdata

# OpenAI API 키 환경 변수로 설정
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
model = "text-embedding-3-small"

```
---

### **3. FAISS 벡터스토어 생성**

- 간단한 텍스트 데이터를 FAISS로 인코딩하여 벡터스토어 생성.

```PY
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# 텍스트 데이터로부터 FAISS 벡터스토어 생성
vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"], 
    embedding=OpenAIEmbeddings(
        openai_api_key=userdata.get('OPENAI_API_KEY'), 
        model="text-embedding-3-small"
    )
)

```
---

### **4. 검색 및 프롬프트 생성**

- 벡터스토어에서 검색 가능한 Retriever와 프롬프트를 생성합니다.

```PY
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 검색 가능한 Retriever 생성
retriever = vectorstore.as_retriever()

# 질문과 컨텍스트 기반의 프롬프트 템플릿 생성
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# Chat 모델 초기화
model = ChatOpenAI()

# 체인을 연결하여 검색 + 질문 응답 파이프라인 구성
retrieval_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

# 예시 실행
retrieval_chain.invoke("where did harrison work?")
# 출력: 'Harrison worked at Kensho.'

```
---

### **5. 문서 데이터 로드 및 벡터화**

- 외부 텍스트 파일을 분할 및 임베딩하여 FAISS에 저장.
```py
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

# 문서 로드 및 분할
loader = TextLoader("./abc.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

# 문서 데이터 벡터화
embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(docs, embeddings)

```

---

### **6. 검색 + Chat 모델을 이용한 질의응답**

- 검색된 문서 내용을 프롬프트와 결합하여 Chat 모델에 전달.

```py
from langchain.schema import HumanMessage, SystemMessage

# Query를 통해 FAISS에서 관련 문서 검색
query = "What numpy functions are there?"
retriever = db.as_retriever()
retrieved_docs = retriever.invoke(query)
retrieved_context = retrieved_docs[0].page_content  # 첫 번째 검색 결과 사용

# 검색 결과와 Query 결합
combined_query = f"Context: {retrieved_context}\nQuery: {query}"

# ChatOpenAI 모델 사용
chat = ChatOpenAI()
response = chat([
    SystemMessage(content="Please use the following information to assist."),
    HumanMessage(content=combined_query)
])

# 응답 출력
print(response.content)

```