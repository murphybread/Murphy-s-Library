---
{"dg-publish":true,"permalink":"/projects/library/400/430/430-10/430-10-a/","noteIcon":"0","created":"2024-02-28T15:40:53.246+09:00","updated":"2024-03-01T00:42:41.122+09:00"}
---

#[[Projects/Library/400/400\|400]]#ML_Engineer_Basic#[[Projects/Library/400/430/430\|430]]#ML_Development_Tools#[[Projects/Library/400/430/430.10/430.10\|430.10]]#Streamlit#[[Projects/Library/400/430/430.10/430.10 a\|430.10 a]]#LLM_RAG_Application_Using_Langchain_and_FAISS

# LLM_RAG_Application_Using_Langchain_and_FAISS

# OpenAI and RAG With FAISS by LangChain
`pip install -q openai langchain tiktoken faiss-cpu pydantic docarray langchain-openai langchain-community boto3 langchain_experimental`

```py
import os


from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough


from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader


from langchain.schema import HumanMessage, SystemMessage

def get_text_response (query):
    
    #Step 1: Loading and splitting documents: 
    loader = TextLoader("./abc.txt")
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)
    
    #Step 2: Embedding generation and FAISS database creation:
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(docs, embeddings)
    
    
    #Step 3: Query the FAISS db to retrieve relevant document content
    query=query
    retriever = db.as_retriever()
    retrieved_docs = retriever.invoke(query)
    retrieved_context = retrieved_docs[0].page_content  # Assume using first doc's content
    
    # Step 4: Combine the retrieved document content with the original query
    combined_query = f"Context: {retrieved_context}\nQuery: {query}"
    
    # Step 5: Use the combined query as a prompt for the ChatOpenAI model
    chat = ChatOpenAI()  # Ensure the ChatOpenAI instance is correctly initialized
    response = chat.invoke([
        SystemMessage(content="Please use the following information to assist."),
        HumanMessage(content=combined_query)
    ])
    
    return response.content 

```

# Streamlit 
#Streamlit

`pip install streamlit`
`streamlit run <file_name>.py [--server.port 8080]`






```py
import streamlit as st #모든 streamlit 명령은 "st" 별칭을 통해 사용할 수 있습니다.
import custom as glib #로컬 라이브러리 스크립트 참조

st.set_page_config(page_title="프롬프트 템플릿") #HTML 제목
st.title("프롬프트 템플릿") #페이지 제목

# noun = st.text_input("명사")
# adjective = st.text_input("형용사")
# verb = st.text_input("동사")



query = st.text_input("질문")
go_button = st.button("Go", type="primary") #기본 버튼 표시하기


if go_button: #버튼이 클릭될 때 이 if 블록의 코드가 실행됩니다.
    
    with st.spinner("Working..."): #이 블록의 코드가 실행되는 동안 스피너를 표시합니다.
        response_content = glib.get_text_response(query) #지원 라이브러리를 통해 모델을 호출합니다.
        st.write(response_content) #응답 콘텐츠를 표시합니다.
```




# Error

kwargs error openai
```
...
  Parameters {'temperature'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. (type=value_error)
```


bedrock

```
ubuntu:~/environment $ python RAG.py 
...
botocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: 4 schema violations found, please reformat your input and try again.

During handling of the above exception, another exception occurred:

...
ValueError: Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: 4 schema violations found, please reformat your input and try again.
```


## There is error in bedrock because that return output different
print(get_text_response("what is your hobby?" , llm_o))
print(get_text_response("what is youre hobby?" , llm_b))

openai
```
ubuntu:~/environment $ python RAG.py 
I'm here to help you with any questions or tasks you have related to programming, data visualization, or any other topic you need assistance with. How can I support you today?

```

Bedrock
```
Traceback (most recent call last):
  File "/home/ubuntu/environment/RAG.py", line 86, in <module>
    print(get_text_response("what is youre hobby?" , llm_b))
  File "/home/ubuntu/environment/RAG.py", line 77, in get_text_response
    return response.content
AttributeError: 'str' object has no attribute 'content'
```



openai와 bedrock이 중요한 차이. response형식

bedrock 단순한 string 반환
openai여러 설정이나 이름 등 부가적인 정보까지 반환
그중에서 dict_key중에서 content를 추출할 필요가 있음

```
ubuntu:~/environment $ python RAG.py 
Response type: <class 'str'>
respone of Bedrock
Params: {}
Response type: <class 'langchain_core.messages.ai.AIMessage'>
Response attributes: dict_keys(['content', 'additional_kwargs', 'type', 'name', 'id', 'example'])
respone of client=<openai.resources.chat.completions.Completions object at 0x7ff4ee8cb820> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff4ee231240> openai_api_key=SecretStr('**********') openai_proxy=''
```


# Different AWS Region
```
  File "/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/bedrock.py", line 451, in _prepare_input_and_invoke
    raise ValueError(f"Error raised by bedrock service: {e}")
ValueError: Error raised by bedrock service: Could not connect to the endpoint URL: "https://bedrock-runtime.ap-northeast-2.amazonaws.com/model/amazon.titan-text-express-v1/invoke"
```

When I succeed default regioin us-east-1, another env conda using apo-northeast-2 default
Explicitly declaring arguments
before
`llm = Bedrock(model_id = model)`
after
`llm = Bedrock(model_id = model, region_name="us-east-1")`


### pip install langchain_experimental
for `from langchain_experimental.text_splitter import SemanticChunker`