---
{"dg-publish":true,"permalink":"/projects/library/400/410/410-20/410-20-b/","noteIcon":"0","created":"2024-02-07T09:40:03.978+09:00","updated":"2024-02-07T09:51:05.471+09:00"}
---

#Maximum_Likelihood_Estimation_MLE_and_Deep_Learning #[[Projects/Library/400/410/410.20/410.20 b\|410.20 b]]
# Maximum Likelihood Estimation MLE

##  Why we should use Log-Likelihood Estimation?
- When the number of data points is small, it's not a problem, but with numbers exceeding hundreds of millions, calculations become impossible for computers.
- When data are independent, log-likelihood converts multiplication into addition, making it computable for computers.
- With gradient descent, tracking maximum likelihood reduces complexity from O(n^2) to O(n).
- Generally, loss functions use gradient descent to optimize using the negative log-likelihood.

정규분포를 따르는 X로부터 최대가능도 추정법으로 모수를 추정한다면?

# Sample Distribution and Sampling Distribution are very different


# Maximum Likelihood Estimation in Deep Learning
손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰된 확률분포의 거리릍 통해 유도
- 총변동거리 Total Variation Distance
- 쿨백-라이블러 발산 KLdivergence
- 쿨백-라이블러 발산(Kullback-Leibler divergence, KL divergence)은 두 확률분포 간의 차이를 측정하는 데 사용되는 비대칭 측정 방법입니다. 이는 특정 확률분포 P를 다른 확률분포 Q로 얼마나 잘 근사할 수 있는지를 수치적으로 나타내며, 정보 이론에서는 이를 정보 손실의 양으로 해석합니다. 즉, Q가 P를 얼마나 잘 대표하는지의 척도로 사용됩니다. DKL​(P∣∣Q)=∑x​P(x)log(Q(x)P(x)​)
- 바슈타인 거리 Wasserstein Distance