---
{"dg-publish":true,"description":"GPU is a high-priority component in ML infrastructure. Of course, if you run out of VRAM, you can't run the model, so there are ways to reduce VRAM, etc. In another article, you can learn why GPU is used in ML infrastructure and how it can be efficient. LLM runs typically have 2 bytes per parameter, so 7B requires 14GB of VRAM for simple math. But of course it depends on the degree of quantization and optimization. Just a rough estimate.","permalink":"/projects/library/100/120/120-00/120-00/","dgPassFrontmatter":true,"noteIcon":"0","created":"2024-04-23T17:29:22.928+09:00","updated":"2024-06-20T02:14:39.774+09:00"}
---

#[[Projects/Library/100/100\|100]]#Infra#[[Projects/Library/100/120/120\|120]]#ML_Engineer_Infra#[[Projects/Library/100/120/120.00/120.00\|120.00]]#GPU