---
{"dg-publish":true,"title":"Helm 사용하기 ArgoCD, Loki, Grafana","description":"Helm을 이용해서 ArgoCD가 AWS 네트워크연결, Loki와 s3연결, Grafana 업데이트에 대한 Helm사용기를 기록한 글입니다.","permalink":"/projects/library/kr/000/020/020-10/kr-020-10-d/","dgPassFrontmatter":true,"noteIcon":"0","created":"2024-12-18T21:57:26.192+09:00","updated":"2024-12-18T21:59:35.693+09:00"}
---

현재 노트: [[Projects/Library/KR/000/020/020.10/KR-020.10 d\|KR-020.10 d]] Helm 사용하기 ArgoCD, Loki, Grafana
상위 분류: [[Projects/Library/KR/000/020/020.10/KR-020.10\|KR-020.10]] Docker,Kubernetes

#Docker #Kubernetes #Helm


# Helm Argocd Trouble shooting

external dns에, AWS LoadBalancer에 Argocd에 ingress 까지 설치가 됐는데도 작동안해서 해결과정
먼저 Route53, ALB생성여부 확인->생성이 안됨
`kubectl get deployments -A` 및 `kubectl get svc -A` 를 통해 네트워크 리소스들 제대로 떳는지 확인-> 제대로떠있음
ingress확인  `kubectl describe ingress argocd-ingress -n argocd`-> 가능한 서브넷이 없다는 에러발견
```
Events:
  Type     Reason            Age                  From     Message
  ----     ------            ----                 ----     -------
  Warning  FailedBuildModel  94s (x17 over 7m4s)  ingress  Failed build model due to couldn't auto-discover subnets: unable to resolve at least one subnet (0 match VPC and tags: [kubernetes.io/role/elb])
```

ingress.yaml에 subnet 지정해주어서 해결
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argocd-ingress
  namespace: argocd
  annotations:
    ...
    alb.ingress.kubernetes.io/subnets: "subnet-00000000, subnet-1111111, subnet-2222222222"
    ...
```


# Loki with Helm


아래의 IAM Role과 Policy의 경우 Loki가 s3를 쓰기위한 역할관 권한입니다. 로그 수집을 로컬로 하는 경우 해당 로그를 찾을때 각 노드에 따로따로 자장되면 에러가 발생합니다.(노드1에는 있지만 노드2에는 없는 로그를 노드2가 로컬에서 찾을때 라던가)
그래서 로그를 적절한 공용 저장소인 오브젝트스토리지에 저장하는 것이 중요합니다.

OIDC를 이용해서 LOKI POD는 service account형태로 AWS IAM의 권한을 사용하게 됩니다.


#AWS

AWS IAM
CHANGE
- 123456784321-> Your AWS Account
- ap-northeast-2 -> Your Region
- A1B2C3D4E5F6078901234567890ABCD -> Your EKS OIDC(You can see in UI or command)
- NAMESPACE -> Loki installed namespace
- SERVICEACCOUNT_NAME -> Service account in values yaml or check from EKS cluster


trustrelationships ROLE 
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::123456784321:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/A1B2C3D4E5F6078901234567890ABCD"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.ap-northeast-2.amazonaws.com/id/A1B2C3D4E5F6078901234567890ABCD:sub": "system:serviceaccount:[NAMESPACE]:[SERVICEACCOUNT_NAME]"
                }
            }
        }
    ]
}
```






s3_loki Policy
Change
- YOUR_BUCKET_NAME -> BUCKETNAME for saving files from loki

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject"
            ],
            "Resource": "arn:aws:s3:::YOUR_BUCKET_NAME*/*"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::YOUR_BUCKET_NAME"
        },
        {
            "Sid": "VisualEditor2",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::YOUR_BUCKET_NAME",
                "arn:aws:s3:::YOUR_BUCKET_NAME/*"
            ]
        }
    ]
}
```





values.yaml
change
- 123456784321-> YOUR AWS account
- CUSTOM_ROLE_NAME -> Name of IAM role just before you create
- <BUCKET_NAME> -> The bucket name you allowed in IAM in the previous step
- <YOUR_REGION> -> YOUR BUCKET. REGION
```
loki:
  auth_enabled: false

  serviceAccount:
    create: true
    name: loki-sa
    namespace: monitoring
    annotations:
      eks.amazonaws.com/role-arn: "arn:aws:iam::123456784321:role/[CUSTOM_ROLE_NAME]"

  extraArgs:
    config.expand-env: "true"

  extraEnv:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP

  config:
    memberlist:
      bind_addr:
        - ${MY_POD_IP}

    schema_config:
      configs:
        - from: 2021-05-12
          store: boltdb-shipper
          object_store: s3
          schema: v11
          index:
            prefix: loki_index_
            period: 24h

    storage_config:
      aws:
        s3: s3://<BUCKET_NAME>
        s3forcepathstyle: true
        bucketnames: <BUCKET_NAME>
        region: <YOUR_REGION>
        insecure: false
        sse_encryption: false
      boltdb_shipper:
        shared_store: s3
        cache_ttl: 24h
        active_index_directory: /data/loki/index
        cache_location: /data/loki/cache
    table_manager:
      retention_deletes_enabled: true
      retention_period: 300h  # Make sure this is correctly a duration, e.g., "720h" for 30 days if needed




grafana:
    enabled: false
    sidecar:
      datasources:
        enabled: true
    image:
      tag:  7.5.17 # <--there is a bug in grafana https://github.com/grafana/loki/issues/8136
      users:
        default_theme: dark
```


Reference
https://community.grafana.com/t/gossip-ring-memberlist-no-private-ip-address-found/52209/7


# 트러블 슈팅 그라파나 비번오류
# Grafana 헬름 차트에서 비밀번호 설정이 잘 동작하지 않는 경우 해결방법 
#Kubernetes #Helm #Grafana


## Audience
- Grafana 헬름 차트 비밀번호 변경했는데도 안되는 경우
- Grafana의 admin의 기본 비밀번호를 바꾸고 싶은 경우
## Overview
현상은 helm으로 grafana배포후 admin 비밀번호를 변경하는데 잘 작동을 안하게 된 케이스.
원인 분석결과 deployment의 env가 덮어쓰는 방식.
사용하는 헬름패키지의 경우 내가 모르는 부분을 파악.
해당 deployment는 secret를 참조하기 때문에, secret 변경후 deployment rollout을 통해 변경된 비밀번호 적용.
즉 변경한 부분이 실제로 다른 값에 덮어씌워져서 적용이 안된 부분을 차근차근 확인 후 실제 반영되는 부분 홛인하여 적용


## Content
### UI 테스트 후 혹시나 하는 마음에 CLI로 시도 후  성공 메시지까지 확인
```
kubectl exec -it pod/<grafana pod name> -n monitoring -c grafana -- grafana-cli admin reset-admin-password <MY PASSWORD>

INFO[05-13|05:43:20] Connecting to DB                         logger=sqlstore dbtype=sqlite3
INFO[05-13|05:43:20] Starting DB migrations                   logger=migrator
INFO[05-13|05:43:20] migrations completed                     logger=migrator performed=0 skipped=279 duration=657.661µs

Admin password changed successfully ✔

```

### 다음 명령어를 통해 admin의 현재 password확인
```
kubectl exec pod/loki-grafana-12345676aws-wc29r -n monitoring -c grafana -- env

PATH=/usr/share/grafana/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=loki-grafana-12345676aws-wc29r
....
GF_SECURITY_ADMIN_USER=admin
GF_SECURITY_ADMIN_PASSWORD=12132142112412
...
```

> 직접적인 원인은 컨테이너에 하드코딩된 변수로 admin password를 인식, 해당 원인은 deployment에 의한 재할당으로 파악


### edit을 통해 deployment 내용 확인. admin-password의 경우 secret를 통해 반영중
`kubectl edit deployment.apps/loki-grafana -n monitoring`
```
 82   도 helm, deployment등에의해 덮어 씌워질 수 있다
그러니 환경변수 등 외부요인도 고려하여 찾아보자
특히 계정정보의 경우 secret과 deployment가 있는 경우 구성 정보를 잘 확인하자



